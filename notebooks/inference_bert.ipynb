{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662ef30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:34:17.383675Z",
     "iopub.status.busy": "2025-05-08T15:34:17.383277Z",
     "iopub.status.idle": "2025-05-08T15:34:28.889494Z",
     "shell.execute_reply": "2025-05-08T15:34:28.888485Z"
    },
    "papermill": {
     "duration": 11.512821,
     "end_time": "2025-05-08T15:34:28.891058",
     "exception": false,
     "start_time": "2025-05-08T15:34:17.378237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Kaggle environment.\n",
      "\n",
      "--- Path Check (Inference Mode) ---\n",
      "Competition Base Path: /kaggle/input/feedback-prize-effectiveness ... Found\n",
      "Train CSV (for LabelEncoder): /kaggle/input/feedback-prize-effectiveness/train.csv ... Found\n",
      "Test CSV: /kaggle/input/feedback-prize-effectiveness/test.csv ... Found\n",
      "Test Essays Dir: /kaggle/input/feedback-prize-effectiveness/test/ ... Found\n",
      "Best Model Directory (for tokenizer, config & model): /kaggle/input/feedback-prize-bert-base-uncased-epoch-2/transformers/default/1/epoch_2 ... Found\n",
      "Best Model State Dict File Path (.pt): /kaggle/input/feedback-prize-bert-base-uncased-epoch-2/transformers/default/1/epoch_2/bert-base-uncased-best.pt ... Found\n",
      "Output Directory: /kaggle/working/ ... Found\n",
      "Submission File Path: /kaggle/working/submission.csv ... NOT FOUND\n",
      "Model Config File Path (config.json): /kaggle/input/feedback-prize-bert-base-uncased-epoch-2/transformers/default/1/epoch_2/config.json ... Found\n",
      "--- End Path Check ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Imports and Constants (Inference Focus) ---\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# AutoConfig is needed for Option 2 model loading\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig \n",
    "from sklearn.preprocessing import LabelEncoder # Needed for EFFECTIVENESS_CLASSES\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import numpy as np \n",
    "import time # For timing inference\n",
    "\n",
    "# --- Environment Detection (Basic) ---\n",
    "IS_KAGGLE_ENV = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "# --- Model & Path Configuration ---\n",
    "MODEL_NAME = 'bert-base-uncased' # Base model name used for architecture if config loading fails\n",
    "MAX_LEN = 512 \n",
    "NUM_LABELS = 3 # Will be updated after label encoding from df_train\n",
    "BATCH_SIZE = 16 # Batch size for inference (can be larger than training)\n",
    "\n",
    "# --- Paths (CRITICAL: Update for Kaggle environment) ---\n",
    "if IS_KAGGLE_ENV:\n",
    "    print(\"Running in Kaggle environment.\")\n",
    "    BASE_PATH = \"/kaggle/input/feedback-prize-effectiveness\"\n",
    "    # *** REPLACE 'your-model-dataset-slug' AND 'epoch_X' with your actual Kaggle dataset slug and model folder ***\n",
    "    KAGGLE_MODEL_INPUT_DIR = \"/kaggle/input/feedback-prize-bert-base-uncased-epoch-2/transformers/default/1/epoch_2\" \n",
    "    \n",
    "    BEST_MODEL_DIR = KAGGLE_MODEL_INPUT_DIR # Directory containing tokenizer, config.json (added manually), and .pt file\n",
    "    BEST_MODEL_FILENAME = f\"{MODEL_NAME}-best.pt\" # Name of the state_dict file\n",
    "    BEST_MODEL_PATH = os.path.join(BEST_MODEL_DIR, BEST_MODEL_FILENAME) # Path to the state_dict file\n",
    "    \n",
    "    OUTPUT_DIR = \"/kaggle/working/\" \n",
    "    SUBMISSION_FILE = os.path.join(OUTPUT_DIR, \"submission.csv\")\n",
    "else:\n",
    "    print(\"Running in local environment (for testing inference).\")\n",
    "    BASE_PATH = \"./feedback-prize-effectiveness/\" \n",
    "    # *** Update this to your local best model directory ***\n",
    "    BEST_MODEL_DIR = \"./models/epoch_2/\" # Example: directory containing tokenizer, config.json, and .pt file\n",
    "    BEST_MODEL_FILENAME = f\"{MODEL_NAME}-best.pt\"\n",
    "    BEST_MODEL_PATH = os.path.join(BEST_MODEL_DIR, BEST_MODEL_FILENAME) \n",
    "    \n",
    "    OUTPUT_DIR = \"./\" # Save submission in current dir for local test\n",
    "    SUBMISSION_FILE = \"submission.csv\" \n",
    "\n",
    "# TRAIN_CSV is needed for LabelEncoder to get EFFECTIVENESS_CLASSES consistently\n",
    "TRAIN_CSV = os.path.join(BASE_PATH, \"train.csv\")\n",
    "TEST_CSV = os.path.join(BASE_PATH, \"test.csv\")\n",
    "# TEST_ESSAYS_DIR is needed for test data\n",
    "TEST_ESSAYS_DIR = os.path.join(BASE_PATH, \"test/\")\n",
    "print(\"\\n--- Path Check (Inference Mode) ---\")\n",
    "paths_to_check = {\n",
    "    \"Competition Base Path\": BASE_PATH,\n",
    "    \"Train CSV (for LabelEncoder)\": TRAIN_CSV,\n",
    "    \"Test CSV\": TEST_CSV,\n",
    "    \"Test Essays Dir\": TEST_ESSAYS_DIR,\n",
    "    \"Best Model Directory (for tokenizer, config & model)\": BEST_MODEL_DIR,\n",
    "    \"Best Model State Dict File Path (.pt)\": BEST_MODEL_PATH, \n",
    "    \"Output Directory\": OUTPUT_DIR, \n",
    "    \"Submission File Path\": SUBMISSION_FILE\n",
    "}\n",
    "# Add config.json path check\n",
    "CONFIG_JSON_PATH = os.path.join(BEST_MODEL_DIR, \"config.json\")\n",
    "paths_to_check[\"Model Config File Path (config.json)\"] = CONFIG_JSON_PATH\n",
    "\n",
    "for name, path_val in paths_to_check.items():\n",
    "    exists = os.path.exists(path_val)\n",
    "    status = \"Found\" if exists else \"NOT FOUND\"\n",
    "    if name == \"Best Model State Dict File Path (.pt)\" and not exists and not IS_KAGGLE_ENV:\n",
    "        status += \" (Expected if model not yet trained/placed)\"\n",
    "    elif name == \"Best Model Directory (for tokenizer, config & model)\" and not exists and IS_KAGGLE_ENV:\n",
    "        status += \" (CRITICAL: This path must exist on Kaggle with your model files!)\"\n",
    "    elif name == \"Model Config File Path (config.json)\" and not exists:\n",
    "         status += \" (CRITICAL for Option 2 loading: Ensure config.json is in the model directory!)\"\n",
    "    print(f\"{name}: {path_val} ... {status}\")\n",
    "\n",
    "print(\"--- End Path Check ---\\n\")\n",
    "if IS_KAGGLE_ENV and \"your-model-dataset-slug\" in KAGGLE_MODEL_INPUT_DIR:\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(\"!!! WARNING: 'KAGGLE_MODEL_INPUT_DIR' still contains placeholder           !!!\")\n",
    "    print(\"!!! 'your-model-dataset-slug/epoch_X'. Update with your Kaggle dataset slug!!!\")\n",
    "    print(\"!!! and the correct model folder path.                                     !!!\")\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")\n",
    "# TRAIN_ESSAYS_DIR is needed if df_train is used beyond LabelEncoder\n",
    "TRAIN_ESSAYS_DIR = os.path.join(BASE_PATH, \"train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23962622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:34:28.898564Z",
     "iopub.status.busy": "2025-05-08T15:34:28.898050Z",
     "iopub.status.idle": "2025-05-08T15:34:28.906683Z",
     "shell.execute_reply": "2025-05-08T15:34:28.905687Z"
    },
    "papermill": {
     "duration": 0.013783,
     "end_time": "2025-05-08T15:34:28.908062",
     "exception": false,
     "start_time": "2025-05-08T15:34:28.894279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Path Check (Inference Mode) ---\n",
      "Competition Base Path: /kaggle/input/feedback-prize-effectiveness ... Found\n",
      "Train CSV (for LabelEncoder): /kaggle/input/feedback-prize-effectiveness/train.csv ... Found\n",
      "Test CSV: /kaggle/input/feedback-prize-effectiveness/test.csv ... Found\n",
      "Test Essays Dir: /kaggle/input/feedback-prize-effectiveness/test/ ... Found\n",
      "Best Model Directory (for tokenizer, config & model): /kaggle/input/feedback-prize-bert-base-uncased-epoch-2/transformers/default/1/epoch_2 ... Found\n",
      "Best Model State Dict File Path (.pt): /kaggle/input/feedback-prize-bert-base-uncased-epoch-2/transformers/default/1/epoch_2/bert-base-uncased-best.pt ... Found\n",
      "Output Directory: /kaggle/working/ ... Found\n",
      "Submission File Path: /kaggle/working/submission.csv ... NOT FOUND\n",
      "Model Config File Path (config.json): /kaggle/input/feedback-prize-bert-base-uncased-epoch-2/transformers/default/1/epoch_2/config.json ... Found\n",
      "--- End Path Check ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Path Checking ---\n",
    "print(\"\\n--- Path Check (Inference Mode) ---\")\n",
    "paths_to_check = {\n",
    "    \"Competition Base Path\": BASE_PATH,\n",
    "    \"Train CSV (for LabelEncoder)\": TRAIN_CSV,\n",
    "    \"Test CSV\": TEST_CSV,\n",
    "    \"Test Essays Dir\": TEST_ESSAYS_DIR,\n",
    "    \"Best Model Directory (for tokenizer, config & model)\": BEST_MODEL_DIR,\n",
    "    \"Best Model State Dict File Path (.pt)\": BEST_MODEL_PATH, \n",
    "    \"Output Directory\": OUTPUT_DIR, \n",
    "    \"Submission File Path\": SUBMISSION_FILE\n",
    "}\n",
    "# Add config.json path check\n",
    "CONFIG_JSON_PATH = os.path.join(BEST_MODEL_DIR, \"config.json\")\n",
    "paths_to_check[\"Model Config File Path (config.json)\"] = CONFIG_JSON_PATH\n",
    "\n",
    "for name, path_val in paths_to_check.items():\n",
    "    exists = os.path.exists(path_val)\n",
    "    status = \"Found\" if exists else \"NOT FOUND\"\n",
    "    if name == \"Best Model State Dict File Path (.pt)\" and not exists and not IS_KAGGLE_ENV:\n",
    "        status += \" (Expected if model not yet trained/placed)\"\n",
    "    elif name == \"Best Model Directory (for tokenizer, config & model)\" and not exists and IS_KAGGLE_ENV:\n",
    "        status += \" (CRITICAL: This path must exist on Kaggle with your model files!)\"\n",
    "    elif name == \"Model Config File Path (config.json)\" and not exists:\n",
    "         status += \" (CRITICAL for Option 2 loading: Ensure config.json is in the model directory!)\"\n",
    "    print(f\"{name}: {path_val} ... {status}\")\n",
    "\n",
    "print(\"--- End Path Check ---\\n\")\n",
    "if IS_KAGGLE_ENV and \"your-model-dataset-slug\" in KAGGLE_MODEL_INPUT_DIR:\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(\"!!! WARNING: 'KAGGLE_MODEL_INPUT_DIR' still contains placeholder           !!!\")\n",
    "    print(\"!!! 'your-model-dataset-slug/epoch_X'. Update with your Kaggle dataset slug!!!\")\n",
    "    print(\"!!! and the correct model folder path.                                     !!!\")\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0805f68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:34:28.915270Z",
     "iopub.status.busy": "2025-05-08T15:34:28.914959Z",
     "iopub.status.idle": "2025-05-08T15:34:28.921261Z",
     "shell.execute_reply": "2025-05-08T15:34:28.920363Z"
    },
    "papermill": {
     "duration": 0.011499,
     "end_time": "2025-05-08T15:34:28.922700",
     "exception": false,
     "start_time": "2025-05-08T15:34:28.911201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Cell 2: Helper functions ---\n",
    "def load_essay_texts(essay_ids, essays_dir):\n",
    "    essay_texts = {}\n",
    "    for essay_id in tqdm(essay_ids, desc=f\"Loading essays from {essays_dir}\"):\n",
    "        essay_path = os.path.join(essays_dir, f\"{essay_id}.txt\")\n",
    "        try:\n",
    "            with open(essay_path, 'r') as f:\n",
    "                essay_texts[essay_id] = f.read()\n",
    "        except FileNotFoundError:\n",
    "            if IS_KAGGLE_ENV and \"test\" in essays_dir.lower():\n",
    "                 print(f\"Info: Test essay file not found {essay_path} (may be normal for sample run)\")\n",
    "            else:\n",
    "                print(f\"Warning: Essay file not found {essay_path}\")\n",
    "            essay_texts[essay_id] = \"\" \n",
    "    return essay_texts\n",
    "\n",
    "def format_time(elapsed_seconds):\n",
    "    elapsed_rounded = int(round(elapsed_seconds))\n",
    "    return str(pd.to_timedelta(elapsed_rounded, unit='s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c20ecf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:34:28.929609Z",
     "iopub.status.busy": "2025-05-08T15:34:28.929263Z",
     "iopub.status.idle": "2025-05-08T15:34:29.238766Z",
     "shell.execute_reply": "2025-05-08T15:34:29.237681Z"
    },
    "papermill": {
     "duration": 0.31453,
     "end_time": "2025-05-08T15:34:29.240242",
     "exception": false,
     "start_time": "2025-05-08T15:34:28.925712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /kaggle/input/feedback-prize-effectiveness/test.csv for inference...\n",
      "Test data shape: (10, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading essays from /kaggle/input/feedback-prize-effectiveness/test/: 100%|██████████| 1/1 [00:00<00:00, 87.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Encoding Mapping (for submission columns):\n",
      "Adequate: 0\n",
      "Effective: 1\n",
      "Ineffective: 2\n",
      "Number of unique labels: 3\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Load Data (Test Data and Train Data for Label Encoding) ---\n",
    "print(f\"Loading {TEST_CSV} for inference...\")\n",
    "df_test_original = pd.read_csv(TEST_CSV) \n",
    "df_test = df_test_original.copy() \n",
    "print(f\"Test data shape: {df_test.shape}\")\n",
    "\n",
    "test_essay_ids = df_test['essay_id'].unique()\n",
    "test_essay_texts_map = load_essay_texts(test_essay_ids, TEST_ESSAYS_DIR)\n",
    "df_test['essay_full_text'] = df_test['essay_id'].map(test_essay_texts_map)\n",
    "df_test['discourse_text'] = df_test['discourse_text'].fillna('')\n",
    "df_test['essay_full_text'] = df_test['essay_full_text'].fillna('')\n",
    "\n",
    "try:\n",
    "    df_train_for_labels = pd.read_csv(TRAIN_CSV, usecols=['discourse_effectiveness'])\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(df_train_for_labels['discourse_effectiveness'])\n",
    "    EFFECTIVENESS_CLASSES = label_encoder.classes_\n",
    "    NUM_LABELS = len(EFFECTIVENESS_CLASSES)\n",
    "    print(\"\\nLabel Encoding Mapping (for submission columns):\")\n",
    "    for i, class_name in enumerate(EFFECTIVENESS_CLASSES):\n",
    "        print(f\"{class_name}: {i}\")\n",
    "    print(f\"Number of unique labels: {NUM_LABELS}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load train.csv or fit LabelEncoder: {e}\")\n",
    "    print(\"Submission file column order might be incorrect. Defining default.\")\n",
    "    EFFECTIVENESS_CLASSES = np.array(['Adequate', 'Effective', 'Ineffective'])\n",
    "    NUM_LABELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ee56aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:34:29.248361Z",
     "iopub.status.busy": "2025-05-08T15:34:29.248061Z",
     "iopub.status.idle": "2025-05-08T15:34:29.343792Z",
     "shell.execute_reply": "2025-05-08T15:34:29.342687Z"
    },
    "papermill": {
     "duration": 0.101457,
     "end_time": "2025-05-08T15:34:29.345213",
     "exception": false,
     "start_time": "2025-05-08T15:34:29.243756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from /kaggle/input/feedback-prize-bert-base-uncased-epoch-2/transformers/default/1/epoch_2\n",
      "Tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 4: Tokenizer Initialization ---\n",
    "# Load tokenizer from the directory where the best model was saved\n",
    "try:\n",
    "    print(f\"Loading tokenizer from {BEST_MODEL_DIR}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BEST_MODEL_DIR)\n",
    "except OSError as e:\n",
    "    print(f\"Could not load tokenizer from {BEST_MODEL_DIR}: {e}\")\n",
    "    print(f\"This is critical for inference. Ensure '{BEST_MODEL_DIR}' contains tokenizer files or adjust path.\")\n",
    "    if IS_KAGGLE_ENV and \"your-model-dataset-slug\" in BEST_MODEL_DIR:\n",
    "         print(\"REMINDER: Update 'your-model-dataset-slug' in KAGGLE_MODEL_INPUT_DIR in Cell 1.\")\n",
    "    raise # Stop execution if tokenizer can't be loaded\n",
    "print(\"Tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "383161aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:34:29.352883Z",
     "iopub.status.busy": "2025-05-08T15:34:29.352588Z",
     "iopub.status.idle": "2025-05-08T15:34:29.359887Z",
     "shell.execute_reply": "2025-05-08T15:34:29.359118Z"
    },
    "papermill": {
     "duration": 0.012826,
     "end_time": "2025-05-08T15:34:29.361372",
     "exception": false,
     "start_time": "2025-05-08T15:34:29.348546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Cell 5: PyTorch Dataset Class (Inference Focus) ---\n",
    "class FeedbackPrizeDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len): \n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        text_A = str(row.discourse_text) if pd.notna(row.discourse_text) and str(row.discourse_text).strip() else \" \"\n",
    "        text_B = str(row.essay_full_text) if pd.notna(row.essay_full_text) and str(row.essay_full_text).strip() else \" \"\n",
    "        \n",
    "        if not text_A.strip(): text_A = \" \" \n",
    "        if not text_B.strip(): text_B = \" \" \n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text_A,\n",
    "            text_B,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length', \n",
    "            truncation='longest_first', \n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True, \n",
    "            return_tensors='pt'    \n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs['token_type_ids'].flatten()\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ff31f7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:34:29.369477Z",
     "iopub.status.busy": "2025-05-08T15:34:29.368849Z",
     "iopub.status.idle": "2025-05-08T15:34:29.378147Z",
     "shell.execute_reply": "2025-05-08T15:34:29.377248Z"
    },
    "papermill": {
     "duration": 0.014663,
     "end_time": "2025-05-08T15:34:29.379555",
     "exception": false,
     "start_time": "2025-05-08T15:34:29.364892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 workers for Test DataLoader.\n",
      "\n",
      "Test DataLoader created: 1 batches.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 6: DataLoader for Test Set ---\n",
    "test_torch_dataset = FeedbackPrizeDataset(df_test, tokenizer, MAX_LEN) \n",
    "\n",
    "try:\n",
    "    num_avail_workers = len(os.sched_getaffinity(0)) // 2 \n",
    "except AttributeError:\n",
    "    num_avail_workers = (os.cpu_count() // 2) if os.cpu_count() and os.cpu_count() > 1 else 0 \n",
    "num_avail_workers = max(0, num_avail_workers) \n",
    "# num_avail_workers = 0 # For debugging\n",
    "print(f\"Using {num_avail_workers} workers for Test DataLoader.\")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_torch_dataset,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=num_avail_workers,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "print(f\"\\nTest DataLoader created: {len(test_dataloader)} batches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224880e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:34:29.387182Z",
     "iopub.status.busy": "2025-05-08T15:34:29.386910Z",
     "iopub.status.idle": "2025-05-08T15:34:58.664119Z",
     "shell.execute_reply": "2025-05-08T15:34:58.662956Z"
    },
    "papermill": {
     "duration": 29.282755,
     "end_time": "2025-05-08T15:34:58.665724",
     "exception": false,
     "start_time": "2025-05-08T15:34:29.382969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target device: cpu\n",
      "Loading config from /kaggle/input/feedback-prize-bert-base-uncased-epoch-2/transformers/default/1/epoch_2\n",
      "Model config loaded successfully.\n",
      "Defining model architecture from loaded config...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 15:34:38.100500: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746718478.313818      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746718478.377293      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture defined.\n",
      "Loading model weights (state_dict) from: /kaggle/input/feedback-prize-bert-base-uncased-epoch-2/transformers/default/1/epoch_2/bert-base-uncased-best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/3649812381.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded successfully.\n",
      "Model is on device and ready for inference.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 7: Load Model Architecture and Weights ---\n",
    "# Load config, then architecture, then state dict separately\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Target device: {device}\")\n",
    "\n",
    "# 1. Load Config\n",
    "try:\n",
    "    print(f\"Loading config from {BEST_MODEL_DIR}\")\n",
    "    # Ensure num_labels is passed correctly if not defined in config.json\n",
    "    config = AutoConfig.from_pretrained(BEST_MODEL_DIR, num_labels=NUM_LABELS) \n",
    "    print(\"Model config loaded successfully.\")\n",
    "except OSError as e:\n",
    "     print(f\"Error loading config.json from {BEST_MODEL_DIR}: {e}\")\n",
    "     print(\"Ensure config.json exists in the model directory (it's needed for Option 2 loading).\")\n",
    "     print(\"You might need to manually add it or use the save_pretrained method during training.\")\n",
    "     raise e\n",
    "\n",
    "# 2. Load Architecture from Config\n",
    "print(\"Defining model architecture from loaded config...\")\n",
    "model_architecture = AutoModelForSequenceClassification.from_config(config)\n",
    "print(\"Model architecture defined.\")\n",
    "\n",
    "# 3. Load State Dict (Weights)\n",
    "loaded_model = model_architecture \n",
    "try:\n",
    "    print(f\"Loading model weights (state_dict) from: {BEST_MODEL_PATH}\")\n",
    "    if not os.path.exists(BEST_MODEL_PATH):\n",
    "        raise FileNotFoundError(f\"Model state_dict file not found at {BEST_MODEL_PATH}\")\n",
    "\n",
    "    loaded_model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
    "    print(\"Model weights loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model weights: {e}\")\n",
    "    print(f\"Attempted to load from: {BEST_MODEL_PATH}\")\n",
    "    print(\"Ensure the BEST_MODEL_PATH is correct and points to a valid .pt state_dict file.\")\n",
    "    print(\"Ensure your model dataset is correctly added to the Kaggle notebook if running on Kaggle.\")\n",
    "    if IS_KAGGLE_ENV and \"your-model-dataset-slug\" in BEST_MODEL_DIR: \n",
    "         print(\"CRITICAL REMINDER: Update 'your-model-dataset-slug' in KAGGLE_MODEL_INPUT_DIR in Cell 1.\")\n",
    "    raise e \n",
    "\n",
    "loaded_model.to(device) \n",
    "loaded_model.eval() \n",
    "print(\"Model is on device and ready for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "131d298d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:34:58.675001Z",
     "iopub.status.busy": "2025-05-08T15:34:58.673966Z",
     "iopub.status.idle": "2025-05-08T15:35:08.175473Z",
     "shell.execute_reply": "2025-05-08T15:35:08.174129Z"
    },
    "papermill": {
     "duration": 9.50757,
     "end_time": "2025-05-08T15:35:08.177202",
     "exception": false,
     "start_time": "2025-05-08T15:34:58.669632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting inference on the test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Batches: 100%|██████████| 1/1 [00:09<00:00,  9.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference on test set completed in: 0 days 00:00:09\n",
      "Shape of predictions_array: (10, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 8: Inference on Test Set ---\n",
    "print(\"\\nStarting inference on the test set...\")\n",
    "all_test_predictions_probs = []\n",
    "t0_inference = time.time()\n",
    "\n",
    "for batch in tqdm(test_dataloader, total=len(test_dataloader), desc=\"Test Batches\"):\n",
    "    b_input_ids = batch['input_ids'].to(device)\n",
    "    b_attention_mask = batch['attention_mask'].to(device)\n",
    "    b_token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        outputs = loaded_model(b_input_ids,\n",
    "                               token_type_ids=b_token_type_ids,\n",
    "                               attention_mask=b_attention_mask)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    logits_cpu = logits.detach().cpu()\n",
    "    probs = torch.softmax(logits_cpu, dim=1).numpy()\n",
    "    all_test_predictions_probs.extend(probs)\n",
    "\n",
    "inference_time = format_time(time.time() - t0_inference) \n",
    "print(f\"Inference on test set completed in: {inference_time}\")\n",
    "\n",
    "predictions_array = np.vstack(all_test_predictions_probs)\n",
    "print(f\"Shape of predictions_array: {predictions_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d6fc92e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:35:08.186783Z",
     "iopub.status.busy": "2025-05-08T15:35:08.186400Z",
     "iopub.status.idle": "2025-05-08T15:35:08.215381Z",
     "shell.execute_reply": "2025-05-08T15:35:08.214176Z"
    },
    "papermill": {
     "duration": 0.035631,
     "end_time": "2025-05-08T15:35:08.216886",
     "exception": false,
     "start_time": "2025-05-08T15:35:08.181255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating submission file...\n",
      "\n",
      "Submission file created: /kaggle/working/submission.csv\n",
      "First 5 rows of submission file:\n",
      "   discourse_id  Ineffective  Adequate  Effective\n",
      "0  a261b6e14276     0.005158  0.286008   0.708834\n",
      "1  5a88900e7dc1     0.035271  0.849891   0.114838\n",
      "2  9790d835736b     0.006935  0.441666   0.551399\n",
      "3  75ce6d68b67b     0.020652  0.578649   0.400700\n",
      "4  93578d946723     0.024025  0.833997   0.141977\n",
      "\n",
      "Submission file saved to: /kaggle/working/submission.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 9: Create Submission File ---\n",
    "print(\"\\nCreating submission file...\")\n",
    "submission_df = pd.DataFrame()\n",
    "submission_df['discourse_id'] = df_test_original['discourse_id']\n",
    "\n",
    "if 'EFFECTIVENESS_CLASSES' not in globals() or len(EFFECTIVENESS_CLASSES) != NUM_LABELS:\n",
    "    print(\"Warning: EFFECTIVENESS_CLASSES not properly defined. Using default for submission columns.\")\n",
    "    EFFECTIVENESS_CLASSES = np.array(['Adequate', 'Effective', 'Ineffective']) # Fallback\n",
    "\n",
    "col_map = {name: i for i, name in enumerate(EFFECTIVENESS_CLASSES)}\n",
    "\n",
    "try:\n",
    "    submission_df['Ineffective'] = predictions_array[:, col_map['Ineffective']]\n",
    "    submission_df['Adequate']    = predictions_array[:, col_map['Adequate']]\n",
    "    submission_df['Effective']   = predictions_array[:, col_map['Effective']]\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError creating submission columns: {e}. Mismatch between EFFECTIVENESS_CLASSES ({EFFECTIVENESS_CLASSES}) and required columns?\")\n",
    "    raise\n",
    "\n",
    "submission_df.to_csv(SUBMISSION_FILE, index=False)\n",
    "print(f\"\\nSubmission file created: {SUBMISSION_FILE}\")\n",
    "print(\"First 5 rows of submission file:\")\n",
    "print(submission_df.head())\n",
    "print(f\"\\nSubmission file saved to: {os.path.abspath(SUBMISSION_FILE)}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 3712025,
     "sourceId": 35308,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 336359,
     "modelInstanceId": 315894,
     "sourceId": 382566,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 59.150535,
   "end_time": "2025-05-08T15:35:11.818038",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-08T15:34:12.667503",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
