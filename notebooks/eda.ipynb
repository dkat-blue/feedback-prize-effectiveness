{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.metrics import log_loss\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5163a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "train_df = pd.read_csv('../input/feedback-prize-effectiveness/train.csv') # Adjusted path for Kaggle\n",
    " # train_df = pd.read_csv('./data/feedback-prize-effectiveness/train.csv') # Local path\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Train data shape: {train_df.shape}\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33876844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column types\n",
    "train_df.info()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "# Basic statistics for text length\n",
    "train_df['text_length'] = train_df['discourse_text'].str.len()\n",
    "print(\"\\nText length statistics:\")\n",
    "print(train_df['text_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63553f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the shortest discourse element\n",
    "shortest = train_df.loc[train_df['text_length'].idxmin()]\n",
    "\n",
    "print(\"\\nShortest discourse element:\")\n",
    "print(f\"Text: '{shortest['discourse_text']}'\")\n",
    "print(f\"Length: {shortest['text_length']} characters\")\n",
    "print(f\"Discourse type: {shortest['discourse_type']}\")\n",
    "print(f\"Effectiveness: {shortest['discourse_effectiveness']}\")\n",
    "print(f\"Essay ID: {shortest['essay_id']}\")\n",
    "\n",
    "# Let's also see other short discourse elements\n",
    "very_short = train_df[train_df['text_length'] < 10].sort_values('text_length')\n",
    "print(\"\\nVery short discourse elements (less than 10 characters):\")\n",
    "print(very_short[['discourse_text', 'text_length', 'discourse_type', 'discourse_effectiveness']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a50332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define outlier thresholds using percentiles\n",
    "lower_bound = train_df['text_length'].quantile(0.01)\n",
    "upper_bound = train_df['text_length'].quantile(0.99)\n",
    "\n",
    "# Filter dataset to remove outliers\n",
    "filtered_df = train_df[(train_df['text_length'] >= lower_bound) & \n",
    "                       (train_df['text_length'] <= upper_bound)]\n",
    "\n",
    "print(f\"Original dataset size: {len(train_df)}\")\n",
    "print(f\"After removing outliers: {len(filtered_df)} ({len(filtered_df)/len(train_df)*100:.1f}% of original)\")\n",
    "print(f\"Removed {len(train_df) - len(filtered_df)} outliers\")\n",
    "\n",
    "# Text length statistics after removing outliers\n",
    "print(\"\\nText length statistics after removing outliers (1-99 percentile):\")\n",
    "print(filtered_df['text_length'].describe())\n",
    "\n",
    "# Get statistics by discourse type\n",
    "print(\"\\nMedian text length by discourse type (after removing outliers):\")\n",
    "median_by_type = filtered_df.groupby('discourse_type')['text_length'].median().sort_values(ascending=False)\n",
    "for discourse_type, median_length in median_by_type.items():\n",
    "    print(f\"{discourse_type}: {median_length:.0f} characters\")\n",
    "\n",
    "# Get statistics by effectiveness\n",
    "print(\"\\nMedian text length by effectiveness (after removing outliers):\")\n",
    "median_by_effectiveness = filtered_df.groupby('discourse_effectiveness')['text_length'].median().sort_values(ascending=False)\n",
    "for effectiveness, median_length in median_by_effectiveness.items():\n",
    "    print(f\"{effectiveness}: {median_length:.0f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-essay-analysis-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load essay texts\n",
    "def load_essay_texts(essay_ids, essays_dir):\n",
    "    essay_texts = {}\n",
    "    for essay_id in tqdm(essay_ids, desc=f\"Loading essays from {essays_dir}\"):\n",
    "        essay_path = os.path.join(essays_dir, f\"{essay_id}.txt\")\n",
    "        try:\n",
    "            with open(essay_path, 'r') as f:\n",
    "                essay_texts[essay_id] = f.read()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Essay file not found {essay_path}\")\n",
    "            essay_texts[essay_id] = \"\" # Provide an empty string if not found\n",
    "    return essay_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda-essay-analysis-md",
   "metadata": {},
   "source": [
    "## Full Essay Text Analysis\n",
    "Now, let's analyze the lengths of the full essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-essay-analysis-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the training essays\n",
    "TRAIN_ESSAYS_DIR = '../input/feedback-prize-effectiveness/train/' # Adjusted path for Kaggle\n",
    "# TRAIN_ESSAYS_DIR = './data/feedback-prize-effectiveness/train/' # Local path\n",
    "\n",
    "# Get unique essay IDs from the training data\n",
    "all_essay_ids = train_df['essay_id'].unique()\n",
    "\n",
    "# Load all essay texts\n",
    "all_essay_texts_map = load_essay_texts(all_essay_ids, TRAIN_ESSAYS_DIR)\n",
    "\n",
    "# Create a DataFrame for essays and their lengths\n",
    "essays_data = []\n",
    "for essay_id, text in all_essay_texts_map.items():\n",
    "    essays_data.append({'essay_id': essay_id, 'essay_text': text, 'essay_length_chars': len(text)})\n",
    "df_essays = pd.DataFrame(essays_data)\n",
    "\n",
    "print(f\"Loaded {len(df_essays)} essays.\")\n",
    "df_essays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-essay-analysis-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics for essay length (characters)\n",
    "print(\"Essay length (characters) statistics:\")\n",
    "print(df_essays['essay_length_chars'].describe())\n",
    "\n",
    "# Add word count analysis for essays\n",
    "df_essays['essay_length_words'] = df_essays['essay_text'].apply(lambda x: len(x.split()))\n",
    "print(\"\\nEssay length (words) statistics:\")\n",
    "print(df_essays['essay_length_words'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-essay-analysis-hist-chars",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot essay length distribution (characters)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df_essays['essay_length_chars'], bins=50, kde=True)\n",
    "plt.title('Distribution of Full Essay Lengths (Characters)')\n",
    "plt.xlabel('Essay Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(df_essays['essay_length_chars'].mean(), color='r', linestyle='--', label=f\"Mean: {df_essays['essay_length_chars'].mean():.0f}\")\n",
    "plt.axvline(df_essays['essay_length_chars'].median(), color='g', linestyle='--', label=f\"Median: {df_essays['essay_length_chars'].median():.0f}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-essay-analysis-hist-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot essay length distribution (words)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df_essays['essay_length_words'], bins=50, kde=True)\n",
    "plt.title('Distribution of Full Essay Lengths (Words)')\n",
    "plt.xlabel('Essay Length (words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(df_essays['essay_length_words'].mean(), color='r', linestyle='--', label=f\"Mean: {df_essays['essay_length_words'].mean():.0f}\")\n",
    "plt.axvline(df_essays['essay_length_words'].median(), color='g', linestyle='--', label=f\"Median: {df_essays['essay_length_words'].median():.0f}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda-essay-token-analysis-md",
   "metadata": {},
   "source": [
    "### Token Length Analysis for Essays\n",
    "To better understand the context length requirements for transformer models, let's analyze essay lengths in terms of tokens. We'll use a simple whitespace tokenizer for a rough estimate, but a proper tokenizer (like from `transformers` library) would give more accurate counts for specific models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-essay-analysis-token-estimation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# It's good practice to use the tokenizer you plan to use for your model for accurate length estimation.\n",
    "# For now, let's pick a common one. If you decide on a specific model later, update this.\n",
    "TOKENIZER_NAME = \"bert-base-uncased\" # Replace with your chosen model if different\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Could not load tokenizer {TOKENIZER_NAME}. Using basic split. Error: {e}\")\n",
    "    # Fallback to simple whitespace split if tokenizer loading fails (e.g. no internet in Kaggle notebook)\n",
    "    tokenizer = lambda text: text.split()\n",
    "\n",
    "def count_tokens(text, tokenizer_func):\n",
    "    if hasattr(tokenizer_func, 'tokenize'): # For Hugging Face tokenizers\n",
    "        return len(tokenizer_func.tokenize(text))\n",
    "    else: # For fallback lambda split()\n",
    "        return len(tokenizer_func(text))\n",
    "\n",
    "df_essays['essay_length_tokens'] = df_essays['essay_text'].apply(lambda x: count_tokens(x, tokenizer))\n",
    "\n",
    "print(\"\\nEssay length (tokens - estimated) statistics:\")\n",
    "print(df_essays['essay_length_tokens'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-essay-analysis-hist-tokens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot essay length distribution (tokens)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df_essays['essay_length_tokens'], bins=50, kde=True)\n",
    "plt.title(f'Distribution of Full Essay Lengths (Tokens - Estimated with {TOKENIZER_NAME if hasattr(tokenizer, \"name_or_path\") else \"whitespace\"})')\n",
    "plt.xlabel('Essay Length (tokens)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(df_essays['essay_length_tokens'].mean(), color='r', linestyle='--', label=f\"Mean: {df_essays['essay_length_tokens'].mean():.0f}\")\n",
    "plt.axvline(df_essays['essay_length_tokens'].median(), color='g', linestyle='--', label=f\"Median: {df_essays['essay_length_tokens'].median():.0f}\")\n",
    "common_max_lengths = [512, 1024, 2048, 4096]\n",
    "for length in common_max_lengths:\n",
    "    plt.axvline(length, color='purple', linestyle=':', alpha=0.7, label=f\"Max Length: {length}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Percentage of essays within common max token lengths\n",
    "print(\"\\nPercentage of essays fitting within common max token lengths:\")\n",
    "for length in common_max_lengths:\n",
    "    percentage = (df_essays['essay_length_tokens'] <= length).mean() * 100\n",
    "    print(f\"<= {length} tokens: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e959067",
   "metadata": {},
   "source": [
    "## Distribution of Text Length\n",
    "Let's visualize the distribution of discourse text lengths using the filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65244197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot discourse text length distribution (filtered)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(filtered_df['text_length'], bins=50, kde=True)\n",
    "plt.title('Distribution of Discourse Text Lengths (1-99 percentile)')\n",
    "plt.xlabel('Text Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d3069f",
   "metadata": {},
   "source": [
    "## Distribution of Discourse Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7f5c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of discourse types\n",
    "discourse_type_counts = train_df['discourse_type'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=discourse_type_counts.index, y=discourse_type_counts.values, palette=\"viridis\")\n",
    "plt.title('Distribution of Discourse Types')\n",
    "plt.xlabel('Discourse Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7539052",
   "metadata": {},
   "source": [
    "## Distribution of Discourse Effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e74623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of discourse effectiveness ratings\n",
    "effectiveness_counts = train_df['discourse_effectiveness'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=effectiveness_counts.index, y=effectiveness_counts.values, palette=\"magma\", order=['Ineffective', 'Adequate', 'Effective'])\n",
    "plt.title('Distribution of Discourse Effectiveness')\n",
    "plt.xlabel('Effectiveness')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f4685a",
   "metadata": {},
   "source": [
    "## Text Length vs. Discourse Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9875276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of text length by discourse type (using filtered data)\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.boxplot(x='discourse_type', y='text_length', data=filtered_df, palette=\"coolwarm\")\n",
    "plt.title('Text Length by Discourse Type (1-99 percentile)')\n",
    "plt.xlabel('Discourse Type')\n",
    "plt.ylabel('Text Length (characters)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464987e0",
   "metadata": {},
   "source": [
    "## Text Length vs. Discourse Effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f6249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of text length by discourse effectiveness (using filtered data)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='discourse_effectiveness', y='text_length', data=filtered_df, palette=\"PuBu\", order=['Ineffective', 'Adequate', 'Effective'])\n",
    "plt.title('Text Length by Discourse Effectiveness (1-99 percentile)')\n",
    "plt.xlabel('Effectiveness')\n",
    "plt.ylabel('Text Length (characters)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1490053c",
   "metadata": {},
   "source": [
    "## Discourse Type vs. Effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7186b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crosstab of discourse type and effectiveness\n",
    "type_effectiveness_ct = pd.crosstab(train_df['discourse_type'], train_df['discourse_effectiveness'], normalize='index') * 100\n",
    "type_effectiveness_ct = type_effectiveness_ct[['Ineffective', 'Adequate', 'Effective']] # Ensure order\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(type_effectiveness_ct, annot=True, fmt='.1f', cmap=\"YlGnBu\")\n",
    "plt.title('Effectiveness Distribution within each Discourse Type (%)')\n",
    "plt.xlabel('Effectiveness')\n",
    "plt.ylabel('Discourse Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1241e3",
   "metadata": {},
   "source": [
    "## Word Clouds\n",
    "Let's generate word clouds for each effectiveness category to see if there are any prominent terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1292c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import STOPWORDS\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"student\", \"students\", \"school\", \"schools\", \"people\", \"think\", \"also\", \"would\", \"could\", \"should\", \"get\", \"make\", \"go\", \"going\", \"many\", \"one\", \"example\", \"another\", \"thing\", \"things\", \"lot\", \"use\", \"need\", \"state\", \"states\", \"country\", \"countries\", \"reason\", \"reasons\", \"opinion\", \"believe\", \"feel\", \"like\", \"really\", \"even\", \"though\", \"however\", \"therefore\", \"furthermore\", \"addition\", \"conclusion\", \"first\", \"second\", \"third\", \"finally\", \"dear\", \"name\", \"electoral\", \"college\", \"venus\", \"face\", \"mars\", \"driverless\", \"cars\", \"car\", \"technology\", \"cell\", \"phone\", \"phones\", \"program\", \"activity\", \"activities\", \"extracurricular\"])\n",
    "\n",
    "def generate_wordcloud(text, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, \n",
    "                          background_color='white', \n",
    "                          stopwords=stopwords,\n",
    "                          min_font_size=10).generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n",
    "\n",
    "for effectiveness_level in ['Ineffective', 'Adequate', 'Effective']:\n",
    "    text = \" \".join(review for review in train_df[train_df['discourse_effectiveness'] == effectiveness_level]['discourse_text'])\n",
    "    generate_wordcloud(text, f'Word Cloud for {effectiveness_level} Discourse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda-conclusions",
   "metadata": {},
   "source": [
    "## EDA Conclusions & Next Steps (Initial)\n",
    "\n",
    "1.  **Discourse Lengths:**\n",
    "    * Discourse elements vary significantly in length. `Evidence` and `Concluding Statement` tend to be the longest, while `Position` and `Claim` are shorter.\n",
    "    * `Effective` discourse tends to be longer than `Ineffective` or `Adequate` discourse, on average.\n",
    "    * There are some very short discourse elements (e.g., 4 characters). These might be noise or require special handling.\n",
    "    * The 99th percentile for discourse length is around 1265 characters. Most are much shorter.\n",
    "\n",
    "2.  **Full Essay Lengths:**\n",
    "    * The mean essay length is around 2800 characters or ~500 words (using whitespace split).\n",
    "    * Using a `bert-base-uncased` tokenizer, the mean token count is around 650-700 tokens, with a median around 600 tokens.\n",
    "    * A significant portion of essays exceeds the standard 512 token limit of many BERT-based models:\n",
    "        * Only about 20-25% of essays fit within 512 tokens.\n",
    "        * Around 75-80% fit within 1024 tokens.\n",
    "        * Over 95% fit within 2048 tokens.\n",
    "    * This strongly suggests that a model with a longer context window (e.g., Longformer, BigBird, RoBERTa with modifications, or newer models like LLaMA variants if allowed and feasible) would be beneficial if we want to incorporate full essay context.\n",
    "\n",
    "3.  **Distributions:**\n",
    "    * `Claim` and `Evidence` are the most frequent discourse types.\n",
    "    * `Adequate` is the most common effectiveness rating, followed by `Effective`, then `Ineffective`.\n",
    "\n",
    "4.  **Relationships:**\n",
    "    * The relationship between discourse type and effectiveness is nuanced. For example, `Evidence` has a good proportion of `Effective` ratings, while `Counterclaim` and `Rebuttal` have higher proportions of `Adequate` or `Ineffective`.\n",
    "\n",
    "5.  **Word Clouds:**\n",
    "    * Word clouds show some differences in prominent (unfiltered by common academic/essay terms) words across effectiveness levels, but deeper NLP analysis (n-grams, TF-IDF) would be needed for more robust insights.\n",
    "\n",
    "### Implications for Modeling (Based on EDA Update):\n",
    "\n",
    "* **Model Choice:** Given that many essays are longer than 512 tokens, using a base BERT model (max length 512) and simply concatenating discourse text with surrounding essay text might truncate a lot of useful information. We should consider models designed for longer sequences if we want to leverage more of the essay context. Examples:\n",
    "    * **Longformer** (e.g., `allenai/longformer-base-4096`)\n",
    "    * **BigBird** (e.g., `google/bigbird-roberta-base`)\n",
    "    * **DeBERTa-v3** (can sometimes handle longer sequences better than BERT, though still typically 512 default)\n",
    "    * If efficiency is a major concern (for the Efficiency Prize track), we might need to be creative with chunking or using hierarchical approaches, or stick to models that are efficient even with longer contexts.\n",
    "\n",
    "* **Input Representation:** The strategy of providing context by surrounding the argument with relevant essay text from both sides is a good idea. The amount of context to include (e.g., fixed number of tokens, sentences, or dynamically determined) will be a key hyperparameter, constrained by the chosen model's max sequence length.\n",
    "\n",
    "* **Validation Strategy:** GroupKFold by `essay_id` is crucial to prevent data leakage, as discourse elements from the same essay are not independent. This is correctly identified as a next step.\n",
    "\n",
    "* **Feature Engineering (Potential):**\n",
    "    * Relative position of the discourse element in the essay.\n",
    "    * Length of the discourse element (raw, or normalized by essay length).\n",
    "    * Interaction features between discourse type and text features.\n",
    "\n",
    "This updated EDA, especially the essay length analysis, reinforces the need to carefully consider models that can handle longer contexts if we want to effectively use the full essay text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
