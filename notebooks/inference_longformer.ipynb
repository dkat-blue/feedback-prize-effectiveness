{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pred-intro",
   "metadata": {},
   "source": [
    "# Feedback Prize Effectiveness: Prediction and Submission (Longformer Ensemble)\n",
    "\n",
    "This notebook is designed for generating predictions on the test set using an ensemble of trained Longformer models (one from each fold of K-Fold cross-validation). It assumes the models and their tokenizers have been saved using `save_pretrained` and are available via a Kaggle Dataset.\n",
    "\n",
    "**Key Steps:**\n",
    "1. **Setup**: Imports, constants, path configurations for all K-Fold models.\n",
    "2. **Data Loading**: Load `test.csv` and corresponding essay texts.\n",
    "3. **Label Encoding Info**: Load `train.csv` minimally to get consistent label class order for submission columns.\n",
    "4. **Dataset and DataLoader**: Prepare the test data for inference (done once).\n",
    "5. **Inference Loop**: Iterate through each saved model fold:\n",
    "   a. Load the tokenizer and model for the current fold.\n",
    "   b. Run predictions on the test set.\n",
    "   c. Store the predicted probabilities.\n",
    "6. **Averaging Predictions**: Average the probabilities from all model folds.\n",
    "7. **Submission File Creation**: Format averaged predictions into `submission.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 1: Imports and Constants (Inference Focus) ---\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.auto import tqdm \n",
    "import torch\n",
    "from transformers import LongformerTokenizerFast, LongformerForSequenceClassification \n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import numpy as np \n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "IS_KAGGLE_ENV = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "MODEL_BASE_NAME = 'longformer-base-4096' \n",
    "MAX_LEN = 1024 \n",
    "NUM_LABELS = 3 \n",
    "BATCH_SIZE = 16 \n",
    "N_FOLDS = 5 # Number of folds used during training\n",
    "AMP_ENABLED = True # Enable AMP for inference if using CUDA\n",
    "\n",
    "if IS_KAGGLE_ENV:\n",
    "    print(\"Running in Kaggle environment.\")\n",
    "    COMPETITION_DATA_PATH = \"/kaggle/input/feedback-prize-effectiveness\"\n",
    "    MODEL_FOLDS_BASE_PATH = \"/kaggle/input/longformer-feedback-prize-ensemble/pytorch/default/1/longformer-feedback-prize/\" # USER VERIFIED THIS PATH\n",
    "    OUTPUT_DIR = \"/kaggle/working/\" \n",
    "else:\n",
    "    print(\"Running in local environment (for testing inference).\")\n",
    "    PROJECT_ROOT = \"../\" \n",
    "    COMPETITION_DATA_PATH = os.path.join(PROJECT_ROOT, \"data/feedback-prize-effectiveness/\") \n",
    "    MODEL_FOLDS_BASE_PATH = os.path.join(PROJECT_ROOT, \"models/\") \n",
    "    OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"outputs/\") \n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "SUBMISSION_FILE = os.path.join(OUTPUT_DIR, \"submission.csv\")\n",
    "TRAIN_CSV = os.path.join(COMPETITION_DATA_PATH, \"train.csv\")\n",
    "TEST_CSV = os.path.join(COMPETITION_DATA_PATH, \"test.csv\")\n",
    "TEST_ESSAYS_DIR = os.path.join(COMPETITION_DATA_PATH, \"test/\")\n",
    "TRAIN_ESSAYS_DIR = os.path.join(COMPETITION_DATA_PATH, \"train/\")\n",
    "\n",
    "MODEL_DIR_PATHS = []\n",
    "for i in range(N_FOLDS):\n",
    "    MODEL_DIR_PATHS.append(os.path.join(MODEL_FOLDS_BASE_PATH, f\"{MODEL_BASE_NAME}-fold-{i}-best\"))\n",
    "\n",
    "print(\"\\n--- Path Check (Inference Mode) ---\")\n",
    "paths_to_check = {\n",
    "    \"Competition Data Path\": COMPETITION_DATA_PATH,\n",
    "    \"Train CSV (for LabelEncoder)\": TRAIN_CSV,\n",
    "    \"Test CSV\": TEST_CSV,\n",
    "    \"Test Essays Dir\": TEST_ESSAYS_DIR,\n",
    "    \"Configured Model Folds Base Path\": MODEL_FOLDS_BASE_PATH,\n",
    "    \"Output Directory\": OUTPUT_DIR\n",
    "}\n",
    "for name, path_val in paths_to_check.items():\n",
    "    exists = os.path.exists(path_val)\n",
    "    status = \"Found\" if exists else \"NOT FOUND\"\n",
    "    print(f\"{name}: {path_val} ... {status}\")\n",
    "\n",
    "print(\"\\nChecking Constructed Model Fold Directories:\")\n",
    "all_fold_paths_exist = True\n",
    "for i, path_val in enumerate(MODEL_DIR_PATHS):\n",
    "    exists = os.path.exists(path_val)\n",
    "    status = \"Found\" if exists else \"NOT FOUND\"\n",
    "    if not exists:\n",
    "        all_fold_paths_exist = False\n",
    "        status += \" (CRITICAL: This path must exist with model and tokenizer files!)\"\n",
    "    print(f\"Model Fold {i} Dir: {path_val} ... {status}\")\n",
    "\n",
    "print(\"--- End Path Check ---\\n\")\n",
    "if not all_fold_paths_exist:\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(\"!!! WARNING: One or more model fold directories were NOT FOUND.                  !!!\")\n",
    "    print(\"!!! Please VERIFY the 'MODEL_FOLDS_BASE_PATH' in Cell 1.                         !!!\")\n",
    "    print(f\"!!! It's currently set to: {MODEL_FOLDS_BASE_PATH}                                 !!!\")\n",
    "    print(\"!!! Ensure this path points to the directory directly CONTAINING your fold folders   !!!\")\n",
    "    print(\"!!! (e.g., 'longformer-base-4096-fold-0-best', etc.).                            !!!\")\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")\n",
    "    raise FileNotFoundError(\"One or more model fold directories not found. Please check paths in Cell 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0805f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 2: Helper functions ---\n",
    "def load_essay_texts(essay_ids, essays_dir):\n",
    "    essay_texts = {}\n",
    "    for essay_id in tqdm(essay_ids, desc=f\"Loading essays from {essays_dir}\"):\n",
    "        essay_path = os.path.join(essays_dir, f\"{essay_id}.txt\")\n",
    "        try:\n",
    "            with open(essay_path, 'r', encoding='utf-8') as f:\n",
    "                essay_texts[essay_id] = f.read()\n",
    "        except FileNotFoundError:\n",
    "            if IS_KAGGLE_ENV and \"test\" in essays_dir.lower():\n",
    "                 print(f\"Info: Test essay file not found {essay_path} (may be normal for sample run)\")\n",
    "            else:\n",
    "                print(f\"Warning: Essay file not found {essay_path}\")\n",
    "            essay_texts[essay_id] = \"\" \n",
    "    return essay_texts\n",
    "\n",
    "def format_time(elapsed_seconds):\n",
    "    elapsed_rounded = int(round(elapsed_seconds))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c20ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: Load Data (Test Data and Train Data for Label Encoding) ---\n",
    "print(f\"Loading {TEST_CSV} for inference...\")\n",
    "df_test_original = pd.read_csv(TEST_CSV) \n",
    "df_test = df_test_original.copy() \n",
    "print(f\"Test data shape: {df_test.shape}\")\n",
    "\n",
    "test_essay_ids = df_test['essay_id'].unique()\n",
    "test_essay_texts_map = load_essay_texts(test_essay_ids, TEST_ESSAYS_DIR)\n",
    "\n",
    "df_test['discourse_text'] = df_test['discourse_text'].fillna('').astype(str)\n",
    "df_test['discourse_type'] = df_test['discourse_type'].fillna('').astype(str)\n",
    "\n",
    "try:\n",
    "    df_train_for_labels = pd.read_csv(TRAIN_CSV, usecols=['discourse_effectiveness'])\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(df_train_for_labels['discourse_effectiveness'])\n",
    "    EFFECTIVENESS_CLASSES = label_encoder.classes_\n",
    "    NUM_LABELS = len(EFFECTIVENESS_CLASSES)\n",
    "    print(\"\\nLabel Encoding Mapping (for submission columns):\")\n",
    "    for i, class_name in enumerate(EFFECTIVENESS_CLASSES):\n",
    "        print(f\"{class_name}: {i}\")\n",
    "    print(f\"Number of unique labels: {NUM_LABELS}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load train.csv or fit LabelEncoder: {e}\")\n",
    "    print(\"Submission file column order might be incorrect. Defining default.\")\n",
    "    EFFECTIVENESS_CLASSES = np.array(['Adequate', 'Effective', 'Ineffective']) \n",
    "    NUM_LABELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383161aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: PyTorch Dataset Class (Inference - using the same as training for consistency) ---\n",
    "class FeedbackPrizeDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, essay_texts_map, has_labels=False):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.has_labels = has_labels\n",
    "        self.essay_texts_map = essay_texts_map\n",
    "        self.cls_token = self.tokenizer.cls_token\n",
    "        self.sep_token = self.tokenizer.sep_token\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def _find_discourse_indices(self, essay_text, discourse_text):\n",
    "        try:\n",
    "            start_idx = essay_text.find(discourse_text)\n",
    "            if start_idx == -1:\n",
    "                start_idx = essay_text.find(discourse_text.strip())\n",
    "                if start_idx != -1:\n",
    "                    discourse_text = discourse_text.strip()\n",
    "            if start_idx != -1:\n",
    "                end_idx = start_idx + len(discourse_text)\n",
    "                return start_idx, end_idx\n",
    "        except Exception:\n",
    "            pass\n",
    "        return -1, -1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        discourse_type_str = str(row.discourse_type).strip()\n",
    "        discourse_text_str = str(row.discourse_text).strip()\n",
    "        essay_id = row.essay_id\n",
    "        essay_full_text_str = self.essay_texts_map.get(essay_id, \"\").strip()\n",
    "\n",
    "        t_type = self.tokenizer.tokenize(discourse_type_str)\n",
    "        t_text = self.tokenizer.tokenize(discourse_text_str)\n",
    "\n",
    "        num_special_tokens = 5 \n",
    "        current_payload_len = len(t_type) + len(t_text)\n",
    "        max_payload_for_type_text = self.max_len - num_special_tokens\n",
    "\n",
    "        if current_payload_len > max_payload_for_type_text:\n",
    "            if len(t_text) > (current_payload_len - max_payload_for_type_text):\n",
    "                t_text = t_text[:len(t_text) - (current_payload_len - max_payload_for_type_text)]\n",
    "            else:\n",
    "                t_text = []\n",
    "                t_type = t_type[:max_payload_for_type_text]\n",
    "            current_payload_len = len(t_type) + len(t_text)\n",
    "        \n",
    "        t_context_before = []\n",
    "        t_context_after = []\n",
    "        remaining_budget_for_context = self.max_len - (current_payload_len + num_special_tokens)\n",
    "\n",
    "        if remaining_budget_for_context > 0 and essay_full_text_str:\n",
    "            start_idx, end_idx = self._find_discourse_indices(essay_full_text_str, discourse_text_str)\n",
    "            if start_idx != -1:\n",
    "                context_before_str = essay_full_text_str[:start_idx].strip()\n",
    "                context_after_str = essay_full_text_str[end_idx:].strip()\n",
    "                budget_for_before = remaining_budget_for_context // 2\n",
    "                budget_for_after = remaining_budget_for_context - budget_for_before\n",
    "                if context_before_str:\n",
    "                    temp_cb_tokens = self.tokenizer.tokenize(context_before_str)\n",
    "                    t_context_before = temp_cb_tokens[-budget_for_before:]\n",
    "                if context_after_str:\n",
    "                    temp_ca_tokens = self.tokenizer.tokenize(context_after_str)\n",
    "                    t_context_after = temp_ca_tokens[:budget_for_after]\n",
    "        \n",
    "        tokens = [self.cls_token] + t_type + [self.sep_token] + t_text + \\\n",
    "                 [self.sep_token] + t_context_before + \\\n",
    "                 [self.sep_token] + t_context_after + [self.sep_token]\n",
    "        \n",
    "        if len(tokens) > self.max_len:\n",
    "            tokens = tokens[:self.max_len -1] + [self.sep_token]\n",
    "        \n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        padding_length = self.max_len - len(input_ids)\n",
    "        if padding_length > 0:\n",
    "            input_ids = input_ids + ([self.pad_token_id] * padding_length)\n",
    "            attention_mask = attention_mask + ([0] * padding_length)\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long)\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff31f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 5: DataLoader for Test Set ---\n",
    "temp_tokenizer_for_dataset = LongformerTokenizerFast.from_pretrained(MODEL_DIR_PATHS[0]) # Load one tokenizer for dataset creation\n",
    "test_torch_dataset = FeedbackPrizeDataset(df_test, temp_tokenizer_for_dataset, MAX_LEN, test_essay_texts_map, has_labels=False) \n",
    "del temp_tokenizer_for_dataset \n",
    "gc.collect()\n",
    "\n",
    "# Set num_workers=0 for Kaggle to avoid multiprocessing issues\n",
    "num_inference_workers = 0 \n",
    "print(f\"Using {num_inference_workers} workers for Test DataLoader.\")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_torch_dataset,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=num_inference_workers,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "print(f\"\\nTest DataLoader created: {len(test_dataloader)} batches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131d298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 6: Inference on Test Set (Looping through Folds) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Target device for inference: {device}\")\n",
    "if device.type == 'cpu' and AMP_ENABLED:\n",
    "    print(\"AMP is set to enabled, but no CUDA device found. AMP will not be used for inference.\")\n",
    "    LOCAL_AMP_ENABLED = False\n",
    "else:\n",
    "    LOCAL_AMP_ENABLED = AMP_ENABLED\n",
    "\n",
    "all_folds_predictions_probs = []\n",
    "overall_inference_start_time = time.time()\n",
    "\n",
    "for fold_num, model_dir_path in enumerate(MODEL_DIR_PATHS):\n",
    "    print(f\"\\n--- Processing Fold {fold_num + 1}/{N_FOLDS} ---\")\n",
    "    print(f\"Loading model from {model_dir_path}\") \n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(model_dir_path):\n",
    "            raise FileNotFoundError(f\"Model directory for fold {fold_num} not found at {model_dir_path}\")\n",
    "        \n",
    "        model_fold = LongformerForSequenceClassification.from_pretrained(model_dir_path, num_labels=NUM_LABELS)\n",
    "        model_fold.to(device)\n",
    "        model_fold.eval()\n",
    "        print(f\"Model for fold {fold_num + 1} loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model for fold {fold_num + 1} from {model_dir_path}: {e}\")\n",
    "        if IS_KAGGLE_ENV and \"your-longformer-model-dataset-slug\" in model_dir_path: \n",
    "             print(\"REMINDER: Ensure 'MODEL_FOLDS_BASE_PATH' in Cell 1 is correctly set to your Kaggle dataset path.\")\n",
    "        raise\n",
    "\n",
    "    fold_predictions_probs = []\n",
    "    fold_inference_start_time = time.time()\n",
    "\n",
    "    for batch in tqdm(test_dataloader, desc=f\"Fold {fold_num+1} Test Batches\"):\n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            # Corrected autocast usage\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(LOCAL_AMP_ENABLED and device.type == 'cuda')):\n",
    "                outputs = model_fold(input_ids=b_input_ids, attention_mask=b_attention_mask)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        logits_cpu = logits.detach().cpu()\n",
    "        probs = torch.softmax(logits_cpu, dim=1).numpy()\n",
    "        fold_predictions_probs.extend(probs)\n",
    "    \n",
    "    all_folds_predictions_probs.append(np.vstack(fold_predictions_probs))\n",
    "    print(f\"Inference for fold {fold_num + 1} completed in: {format_time(time.time() - fold_inference_start_time)}\")\n",
    "    \n",
    "    del model_fold \n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nOverall inference for {N_FOLDS} folds completed in: {format_time(time.time() - overall_inference_start_time)}\")\n",
    "\n",
    "if all_folds_predictions_probs:\n",
    "    averaged_predictions_array = np.mean(all_folds_predictions_probs, axis=0)\n",
    "    print(f\"Shape of averaged_predictions_array: {averaged_predictions_array.shape}\")\n",
    "else:\n",
    "    print(\"No predictions were generated. Check for errors in the loop.\")\n",
    "    averaged_predictions_array = np.zeros((len(df_test), NUM_LABELS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6fc92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 7: Create Submission File ---\n",
    "print(\"\\nCreating submission file...\")\n",
    "submission_df = pd.DataFrame()\n",
    "submission_df['discourse_id'] = df_test_original['discourse_id']\n",
    "\n",
    "if 'EFFECTIVENESS_CLASSES' not in globals() or len(EFFECTIVENESS_CLASSES) != NUM_LABELS:\n",
    "    print(\"Warning: EFFECTIVENESS_CLASSES not properly defined. Using default for submission columns.\")\n",
    "    EFFECTIVENESS_CLASSES = np.array(['Adequate', 'Effective', 'Ineffective'])\n",
    "\n",
    "col_map = {name: i for i, name in enumerate(EFFECTIVENESS_CLASSES)}\n",
    "submission_cols_ordered = ['Ineffective', 'Adequate', 'Effective'] \n",
    "\n",
    "try:\n",
    "    for col_name in submission_cols_ordered:\n",
    "        if col_name in col_map:\n",
    "            submission_df[col_name] = averaged_predictions_array[:, col_map[col_name]]\n",
    "        else:\n",
    "            raise KeyError(f\"Column '{col_name}' not found in label encoder mapping: {EFFECTIVENESS_CLASSES}\")\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError creating submission columns: {e}.\")\n",
    "    print(\"Ensure your label_encoder.classes_ order during training matches the expected submission columns or adjust mapping here.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during submission file creation: {e}\")\n",
    "    raise\n",
    "\n",
    "submission_df.to_csv(SUBMISSION_FILE, index=False)\n",
    "print(f\"\\nSubmission file created: {SUBMISSION_FILE}\")\n",
    "print(\"First 5 rows of submission file:\")\n",
    "print(submission_df.head())\n",
    "print(f\"\\nSubmission file saved to: {os.path.abspath(SUBMISSION_FILE)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
